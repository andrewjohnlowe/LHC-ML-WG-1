<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Machine learning for particle physics using R</title>
  <meta name="description" content="">
  <meta name="author" content="Andrew John Lowe">
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <link rel="stylesheet" href="libraries/frameworks/revealjs/css/reveal.min.css">
  <link rel="stylesheet" href="libraries/frameworks/revealjs/css/theme/sky.css" id="theme">
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/default.css" id="theme">
  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->  <link rel="stylesheet" href = "assets/css/stylesheet.css">

</head>
<body>
  <div class="reveal">
    <div class="slides">
      <section class='' data-state='' id='slide-1'>
  
  <style>
.reveal h1,
.reveal h2,
.reveal h3,
.reveal h4,
.reveal h5,
.reveal h6 {
font-family: "Quicksand", sans-serif;
letter-spacing: -0.08em;
text-transform: uppercase;
text-shadow: none; }
.reveal {
text-align: left;
font-family: "Open Sans", sans-serif;
}
.noborder .reveal section img {
background:none; 
border:none; 
box-shadow:none;
}
.small-code pre code {
font-size: 1em;
}
.midcenter {
position: fixed;
top: 50%;
left: 50%;
}
.footer {
color: grey; background: none;
text-align:left; width:100%;
}
.left {
text-align:left;
}
.right {
text-align:right;
}
table.mytable {
border: none;
width: 100%;
border-collapse: collapse;
font-size: 45px;
line-height: 50px;
color: black;
}
.blink {
animation: blink 1s steps(5, start) infinite;
-webkit-animation: blink 1s steps(5, start) infinite;
}
@keyframes blink {
to {
visibility: hidden;
}
}
@-webkit-keyframes blink {
to {
visibility: hidden;
}
}
</style>

<h2>Machine learning for particle physics using R</h2>

<p><br></p>

<h4>Andrew John Lowe</h4>

<p><br></p>

<h4>Wigner Research Centre for Physics,</h4>

<h4>Hungarian Academy of Sciences</h4>

<script src="jquery.min.js"></script>

</section>
<section class='' data-state='' id='slide-2'>
  <h3>Introduction: about this talk</h3>
  <ul>
<li>I&#39;m shamelessly recycling slides that I presented at a data science conference in Budapest two weeks ago, so what follows will contain little (if any) physics</li>
<li>As this is the inaugaral meeting of the LHC machine learning WG, let&#39;s start with the absolute basics: <strong>tools</strong></li>
<li>I&#39;ll talk about how using <strong>R</strong> has made it easier for me to ask more complex questions from my data than I would have been able to otherwise</li>
<li>I&#39;m not advocating that everyone should switch to R</li>
<li>Hopefully this talk will be complementary to discussions of ROOTR and RMVA</li>
</ul>

</section>
<section class='' data-state='' id='slide-3'>
  <h3>Tools</h3>
  <ul>
<li>Recognise that when you chose a specific analysis tool or framework, you are making a choice that invariably involves some kind of trade-off

<ul>
<li>Not always obvious what this compromise is</li>
<li>Do you have the right tool for the job?</li>
</ul></li>
<li><em>When your only tool is a hammer, everything looks like a nail</em>

<ul>
<li>Generally a good idea to have some familiarity with more than one data analysis tool</li>
</ul></li>
<li>What analysis tools should our students learn?

<ul>
<li>Given that PhD \(\neq\) academic career, what knowledge of tools would provide a good RoI and best equip students with the technical skills they need in a job outside HEP?</li>
</ul></li>
<li>What is available to particle physicists?</li>
</ul>

</section>
<section>
   <section class='' data-state=''>
    
    <p><img src="root6-banner.png" alt="ROOT"></p>

<ul>
<li>For experimental particle physics, <a href="http:://root.cern.ch">ROOT</a> is the ubiquitous data analysis tool, and has been for the last 20 years old</li>
<li>Command language: CINT (&quot;interpreted C++&quot;) or Python

<ul>
<li>Small data: work interactively or run macros</li>
<li>Big data: compile with ROOT libraries, run on Grid</li>
</ul></li>
<li>Data format optimised for large data sets</li>
<li>Complex algorithms are difficult to do interactively</li>
<li>End up writing huge C++ programs</li>
<li>Lots of tweaking, endless edit-compile-run loops</li>
<li>The best choice for prototyping new methods?
<br></li>
</ul>

<p class='footer'><small>See <em>Highlights and Analysis of the Answers to the ROOT Users&#39; Survey</em>, <a href="http://indico.cern.ch/event/349459/">&quot;ROOT Turns 20&quot; Users&#39; Workshop</a>, 15-18 September 2015, Saas-Fee, Switzerland</small></p>

    <aside class='notes'>
      
    </aside>
   </section>
</section>
<section class='' data-state='' id='slide-5'>
  <h2>What happened outside HEP in the past 20 years?</h2>
  
</section>
<section class='' data-state='' id='slide-6'>
  
  <p><img src="Big-Data-Landscape1.png" alt="Big Data Landscape"></p>

</section>
<section class='' data-state='' id='slide-7'>
  <h3>On C++ and data analysis</h3>
  <ul>
<li>Is C++ a good choice for data analysis?

<ul>
<li>Spend days coding something that runs in minutes <strong>or</strong></li>
<li>Write something in a couple of hours that will run during your lunch break?</li>
<li>Which will get you your answer faster? What strategy will help you define where you should be focusing your efforts and which paths lead to dead-ends?</li>
</ul></li>
<li><a href="https://www.youtube.com/watch?v=LR8fQiskYII">Larry Wall</a>, creator of Perl (speaking about differences in the number of lines of code needed to accomplish the same task using different languages):</li>
</ul>

<p><br>
<q>You can eat a one-pound stake, or a 100 pounds of shoe leather, and you feel a greater sense of accomplishment after the shoe leather, but maybe there&#39;s some downsides...</q></p>

</section>
<section class='' data-state='' id='slide-8'>
  <h3>What I want in my tool box:</h3>
  <p><img src="assets/fig/screwdriver-1.png" title="plot of chunk screwdriver" alt="plot of chunk screwdriver" style="display: block; margin: auto;" /></p>

</section>
<section class='' data-state='' id='slide-9'>
  <h3>Sometimes I want to use:</h3>
  <p><img src="assets/fig/scalpel-1.png" title="plot of chunk scalpel" alt="plot of chunk scalpel" style="display: block; margin: auto;" /></p>

</section>
<section class='' data-state='' id='slide-10'>
  <h3>Or sometimes:</h3>
  <p><img src="assets/fig/hammer-1.png" title="plot of chunk hammer" alt="plot of chunk hammer" style="display: block; margin: auto;" /></p>

</section>
<section class='' data-state='' id='slide-11'>
  <h3>What ROOT provides</h3>
  <p><img src="assets/fig/swiss-1.png" title="plot of chunk swiss" alt="plot of chunk swiss" style="display: block; margin: auto;" />
There are positives and negatives here...</p>

</section>
<section class='' data-state='' id='slide-12'>
  <h3>Why did I choose R?</h3>
  <ul>
<li>Chief among those were the need for fast prototyping and high-level abstractions that let me concentrate on what I wanted to achieve, rather than on the mechanics and the highly-granular details of how I might do it</li>
<li>Incredibly easy to express what I want to achieve</li>
<li>Exponentially-growing number of add-on packages</li>
<li>Latest machine learning algorithms are available</li>
<li>About 2 million R users worldwide\(^*\); technical questions are answered extremely quickly (if not already)</li>
<li>Not as fast as C++, but <strong>my goal is to quickly test new ideas rather than implement a final model</strong></li>
<li>Beautiful plots</li>
<li>Fun to work with ‚ò∫</li>
</ul>

<p class='footer'><br><small>* <a href="http://www.inside-r.org/what-is-r">http://www.inside-r.org/what-is-r</a></small></p>

</section>
<section class='' data-state='' id='slide-13'>
  <h3>Downsides to using R?</h3>
  <ul>
<li>Can be very slow

<ul>
<li>Base R is single-threaded</li>
<li>Unlikely to do a full HEP analysis with all the data in R!</li>
<li>But fine for looking at small chunks of data</li>
<li>Avoid <em>for</em> loops</li>
</ul></li>
<li>Your data has to fit into RAM

<ul>
<li>But not always strictly true

<ul>
<li><em>Revolution R Enterprise</em> provides the <strong>RevoScaleR</strong> package and XDF file format for Big Data</li>
<li><em>H2O.ai</em> enables users to import data via a reference to an object in a distributed key-value store</li>
<li>Other packages: <strong>ff</strong>, <strong>bigmemory</strong>...</li>
</ul></li>
</ul></li>
</ul>

</section>
<section class='' data-state='' id='slide-14'>
  <h3>Getting ROOT data into R</h3>
  <ul>
<li><strong>RootTreeToR</strong> enables users to import ROOT data into R</li>
<li>Written by Adam Lyon (FNAL), presented at <a href="http://user2007.org/">useR! 2007</a></li>
<li><a href="https://cdcvs.fnal.gov/redmine/projects/roottreetor">cdcvs.fnal.gov/redmine/projects/roottreetor</a></li>
<li>Requires ROOT to be installed, but no need to run ROOT</li>
</ul>

<pre><code class="r"># Open and load ROOT tree:
rt &lt;- openRootChain(&quot;TreeName&quot;, &quot;FileName&quot;)
N &lt;- nEntries(rt) # number of rows of data
# Names of branches:
branches &lt;- RootTreeToR::getNames(rt)
# Read in a subset of branches (varsList), M rows:
df &lt;- toR(rt, varsList, nEntries=M)
</code></pre>

<ul>
<li>Recently became aware of <strong>ROOTR</strong>, and I look forward to playing with that in the very near future</li>
</ul>

</section>
<section class='' data-state='' id='slide-15'>
  <h3>Getting and cleaning data in R</h3>
  <ul>
<li><strong>data.table</strong> is extremely useful here:

<ul>
<li><strong>fread</strong> found to be at least twice as fast as other methods I tried for importing my data</li>
<li>Helps me clean and filter my data and is super-fast, especially when using keys:</li>
</ul></li>
</ul>

<pre><code>setkey(DT, numTracks) # Set number of particle tracks to be the key
DT &lt;- DT[!.(1)] # Remove all single-track jets
DT[, (bad.cols) := NULL] # Remove junk columns
</code></pre>

<ul>
<li><strong>digest</strong> is also useful for removing duplicate columns by fast comparison of hashes:</li>
</ul>

<pre><code>duplicate.columns &lt;- names(DT)[duplicated(lapply(DT, digest))]
DT[, (duplicate.columns) := NULL]
</code></pre>

<ul>
<li><strong>knitr</strong> and R Markdown used everywhere to document process; broke workflow into chunks, one R Markdown file for each, saving intermediate results along the way</li>
</ul>

</section>
<section class='' data-state='' id='slide-16'>
  <h3>More data munging</h3>
  <ul>
<li>To give me some extra space in RAM to work I used <strong>SOAR</strong> (stored object caches for R):</li>
</ul>

<pre><code>Sys.setenv(R_LOCAL_CACHE = &quot;soar_cache&quot;)
Store(DT) # data.table now stored as RData file on disk and out of RAM
</code></pre>

<ul>
<li><strong>caret</strong> also provides some useful data-munging; I could reduce the size of my data by more than 50% with a conservative cut on correlations between features:</li>
</ul>

<pre><code>highly.correlated &lt;- findCorrelation(
  cor(DT[,-ncol(DT), with = FALSE], method = &quot;pearson&quot;),
  cutoff = 0.95, names = TRUE)
</code></pre>

<ul>
<li>Removing duplicate and highly correlated features was critical for enabling my data to fit in RAM

<ul>
<li>To preserve interpretability, I prefer to choose which features to retain instead of letting <strong>caret</strong> pick features that might have less explanatory value</li>
</ul></li>
</ul>

</section>
<section class='' data-state='' id='slide-17'>
  <h3>Feature ranking &amp; selection</h3>
  <ul>
<li>How should we find the features that provide the best discrimination between the processes or physics entities that we wish to classify?</li>
<li>Given a newly-proposed discriminant variable, how can we rank this new variable against those we already know?</li>
<li>We can use domain knowledge to drill down to what are believed to be the best discriminants; observables that:

<ul>
<li>Can explain most of the variance in the data</li>
<li>Are minimally correlated with each other</li>
<li>Provide the best predictive power</li>
</ul></li>
<li>How to optimally search the feature space? (Manual inspection may be impractical for a large feature set)</li>
</ul>

</section>
<section>
   <section class='' data-state=''>
    <h3>Problems of too many features</h3>
    <p><strong>Or: why don&#39;t we throw everything into a boosted decision tree or neural net and hope for the best?</strong></p>

<ul>
<li>Correlated features can skew prediction</li>
<li>Irrelevant features (not correlated to class variable) cause unnecessary blowup of the model space</li>
<li>Irrelevant features can drown the information provided by informative features in noise</li>
<li>Irrelevant features in a model reduce its explanatory value</li>
<li><em>Training may be slower and more computationally expensive</em></li>
<li>Increased risk of overfitting</li>
</ul>

    <aside class='notes'>
      
    </aside>
   </section>
   <section class='' data-state=''>
    <h3>Redundant &amp; irrelevant features</h3>

<p class='left'>What should we do when it is likely that the data contains many redundant or irrelevant features?</p>

<ul>
<li><strong>Redundant features</strong> are those which provide no more information than the currently selected features</li>
<li><strong>Irrelevant features</strong> provide no useful information in any context</li>
</ul>

    <aside class='notes'>
      
    </aside>
   </section>
</section>
<section>
   <section class='' data-state=''>
    <h3>Feature selection methods</h3>
    <p class='left'>Several methods in R for feature ranking and selection:</p>

<ul>
<li>Iteratively remove features shown by a statistical test to be less relevant than random probes: the <em>Boruta</em> algorithm\(^*\)</li>
<li>Rank by <em>information gain</em> (Kullback‚ÄìLeibler divergence)\(^\dagger\) or <em>Gini impurity</em></li>
<li><em>Correlation Feature Selection</em> (CFS)\(^\dagger\)</li>
<li><em>Recursive Feature Elimination</em> (RFE, Backwards Selection)\(^\ddagger\)</li>
<li><em>Simulated annealing</em>\(^\ddagger\)</li>
<li><em>Genetic algorithms</em>\(^\ddagger\)</li>
<li>Many classifiers will output variable importance
<br></li>
<li><strong>Tried all of these with varying levels of success</strong>

<ul>
<li>Speed of some methods limits their utility somewhat</li>
</ul></li>
</ul>

<p class='footer'><small>* <strong>Boruta</strong>, \(\dagger\) <strong>FSelector</strong>, \(\ddagger\) <strong>caret</strong></small></p>

    <aside class='notes'>
      
    </aside>
   </section>
   <section class='' data-state=''>
    <h3>Boruta</h3>

<p><small>
The basic principle, in a nutshell:</p>

<ul>
<li>Boruta algorithm is a wrapper built around the <em>random forest</em> classification algorithm 

<ul>
<li>Random forests are an ensemble learning method for classification (and regression) that operate by stochastically growing a forest of decision trees; each tree is grown in such a way that at each split only a random subset of all features is considered</li>
</ul></li>
<li>The importance measure of an attribute is obtained as the loss of classification accuracy caused by the random permutation of feature values between objects</li>
<li>It is computed separately for all trees in the forest which use a given feature for classification</li>
<li>Then the average and standard deviation of the accuracy loss are computed</li>
<li>Claims to be robust against &quot;selection bias&quot;\(^*\)</li>
</ul>

<p></small></p>

<p class='footer'><small>* <em>Selection bias in gene extraction on the basis of microarray gene-expression data</em>, Christophe Ambroise and Geoffrey J. McLachlan, PNAS vol. 99, No. 10 (2002)</small></p>

    <aside class='notes'>
      
    </aside>
   </section>
   <section class='' data-state=''>
    <h3>Information gain</h3>

<p><small></p>

<ul>
<li>Information gain is based on the concept of entropy from information theory and is commonly used to decide which features to use when growing a decision tree<br><br></li>
</ul>

<p>\[
Entropy = - \sum_{i}{p_i}{\log_{2}}{p_i}
\]</p>

<ul>
<li>In machine learning, this concept can be used to define a preferred sequence of attributes to investigate to most rapidly classify an item</li>
<li>Such a sequence is called a decision tree</li>
<li>At each level, the feature with the highest information gain is chosen</li>
<li>An alternative measure of &quot;node impurity&quot; commonly used in decision tree learning is the Gini impurity:<br><br></li>
</ul>

<p>\[1 - \sum_{i}{p_i}^2\]</p>

<p></small></p>

    <aside class='notes'>
      
    </aside>
   </section>
   <section class='' data-state=''>
    <h3>Recursive Feature Elimination</h3>

<p><small></p>

<ul>
<li>First, the algorithm fits the model to all predictors

<ul>
<li>I used a <em>random forest</em> for the model</li>
</ul></li>
<li>Each predictor is ranked using its importance to the model</li>
<li>Let \(S\) be a sequence of ordered numbers which are candidate values for the number of predictors to retain (\(S_1\) \(>\) \(S_2\), \(\dots\))</li>
<li>At each iteration of feature selection, the \(S_i\) top ranked predictors are retained, the model is refit and performance is assessed</li>
<li>The value of \(S_i\) with the best performance is determined and the top \(S_i\) predictors are used to fit the final model</li>
<li>To minimise the possibility of selection bias, I performed k-fold cross-validation during training with ten folds</li>
</ul>

<p></small></p>

    <aside class='notes'>
      
    </aside>
   </section>
   <section class='' data-state=''>
    <h3>Correlation Feature Selection</h3>

<p><small></p>

<ul>
<li><p>The Correlation Feature Selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: <em>&quot;Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other&quot;</em></p></li>
<li><p>The following equation gives the merit of a feature subset \(S\) consisting of \(k\) features:<br><br></p></li>
</ul>

<p>\[
Merit_{S_{k}} = \frac{k\overline{r_{cf}}}{\sqrt{k+k(k-1)\overline{r_{ff}}}}
\]</p>

<ul>
<li>where \(\overline{r_{cf}}\) is the average value of all feature-classification correlations, and \(\overline{r_{ff}}\) is the average value of all feature-feature correlations. These variables are referred to as correlations, but are not necessarily Pearson&#39;s correlation coefficient or Spearman&#39;s \(\rho\).</li>
</ul>

<p></small></p>

    <aside class='notes'>
      
    </aside>
   </section>
</section>
<section class='' data-state='' id='slide-20'>
  <h3>Summary</h3>
  <ul>
<li>It&#39;s often said that 80% of data analysis is spent on data munging\(^*\) \(-\) this has certainly been true in my case</li>
<li>However, I&#39;ve found a good set of tools for streamlining this process; I&#39;ve shared what I found most useful here</li>
<li>To the best of my knowledge, nobody has tried to do a particle physics analysis entirely in R before

<ul>
<li>Problems with large data, but workarounds exist</li>
<li>Insights gained will be valuable for helping me decide where to direct my efforts later when building a final model</li>
</ul></li>
<li>I didn&#39;t have to spend time writing a ton of code or worrying about dangling pointers, <em>etc.</em></li>
<li><strong>R lets me focus on achieving the goals of my analysis</strong>
<br></li>
</ul>

<p class='footer'><small>* <em>Exploratory Data Mining and Data Cleaning</em>, Dasu T, Johnson T (2003), John Wiley &amp; Sons</small></p>

</section>
<section>
   <section class='' data-state=''>
    <h3>About these slides</h3>
    <ul>
<li>Like these slides? I made them in R using RStudio!</li>
<li>Gratuitous eye-candy comes for free</li>
<li>You can mix HTML, CSS, Javascript, \(\LaTeX\), Unicode, Markdown, movies and R code chunks that evaluate when you <em>knit</em> the slides/document:

<ul>
<li>This is HTML and CSS: <span class="blink">blink tag!</span></li>
<li>This is \(\LaTeX\): \(i \hbar \gamma^\mu \partial_\mu \psi - mc \psi = 0\)</li>
<li>This is Unicode: üê±</li>
<li>Here&#39;s some embedded pseudo-analysis code:</li>
</ul></li>
</ul>

<pre><code class="r">print(pi)
</code></pre>

<pre><code>## [1] 3.141593
</code></pre>

<p><br>
Look down there: \(\downarrow\)</p>

    <aside class='notes'>
      
    </aside>
   </section>
   <section class='' data-state=''>
    <h3>Bonus slides can be here</h3>

<p><strong>This is a vertical slide</strong></p>

<p>Give more detailed information in a <em>basement level</em> instead of at the end of a long linear set of slides</p>

<p>Keep going: \(\downarrow\)</p>

    <aside class='notes'>
      
    </aside>
   </section>
   <section class='' data-state=''>
    <h3>Dynamic theme switcher</h3>

<p><em>In case you didn&#39;t like the current slide theme...</em></p>

<p><a href="?theme=sky#/themes">Sky</a> - <a href="?theme=beige#/themes">Beige</a> - <a href="?theme=simple#/themes">Simple</a> - <a href="?theme=serif#/themes">Serif</a> - <a href="?theme=night#/themes">Night</a> - <a href="?theme=default#/themes">Default</a> - <a href="?theme=solarized#/themes">Solarized</a> - <a href="?theme=moon#/themes">Moon</a></p>

    <aside class='notes'>
      
    </aside>
   </section>
</section>
<section class='' data-state='' id='slide-22'>
  <h2>Thanks!</h2>
  
</section>
    </div>
  </div>
</body>
  <script src="libraries/frameworks/revealjs/lib/js/head.min.js"></script>
  <script src="libraries/frameworks/revealjs/js/reveal.min.js"></script>
  <script>
  // Full list of configuration options available here:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,
    theme: Reveal.getQueryHash().theme || 'sky', 
    transition: Reveal.getQueryHash().transition || 'rotate', 
    dependencies: [
    // Cross-browser shim that fully implements classList -
    // https://github.com/eligrey/classList.js/
      { src: 'libraries/frameworks/revealjs/lib/js/classList.js', condition: function() { return !document.body.classList;}},
      // Zoom in and out with Alt+click
      { src: 'libraries/frameworks/revealjs/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      // Speaker notes
      { src: 'libraries/frameworks/revealjs/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
      // Remote control your reveal.js presentation using a touch device
      //{ src: 'libraries/frameworks/revealjs/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
      ]
  });
  </script>  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
 

</html>